import pandas as pd
train = pd.read_csv('C:\\Users\\USER\\Downloads\\ê¸°ì—…ì„±ê³µ\\train.csv')
test = pd.read_csv('C:\\Users\\USER\\Downloads\\ê¸°ì—…ì„±ê³µ\\test.csv')

#eda 
#ê¸°ì´ˆí†µê³„ëŸ‰ í™•ì¸
print(train.describe())

#íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë¶„í¬ í™•ì¸
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(train['ì„±ê³µí™•ë¥ '], kde=True)
plt.xlabel('success percentage')
plt.show()

#ìƒê´€ê³„ìˆ˜ í™•ì¸
cor = train.drop(['ID','êµ­ê°€','ë¶„ì•¼','íˆ¬ìë‹¨ê³„','ì¸ìˆ˜ì—¬ë¶€','ìƒì¥ì—¬ë¶€','ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'], axis=1).corr()['ì„±ê³µí™•ë¥ '].sort_values()
print(cor)

#ê²°ì¸¡ì¹˜ í™•ì¸
print(train.isnull().sum().sort_values(ascending=False))

#ë²”ì£¼í˜• ë¶„í¬ í™•ì¸
categorical = ['êµ­ê°€','ë¶„ì•¼','íˆ¬ìë‹¨ê³„','ì¸ìˆ˜ì—¬ë¶€','ìƒì¥ì—¬ë¶€','ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)']
for col in categorical:
    print(f"\nğŸ”¹ {col} value counts:")
    print(train[col].value_counts(dropna=False))

#ê²°ì¸¡ì¹˜ ì²˜ë¦¬
train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].fillna(train['ê³ ê°ìˆ˜(ë°±ë§Œëª…)'].median(), inplace=True)
train['ì§ì› ìˆ˜'].fillna(train['ì§ì› ìˆ˜'].median(), inplace=True)
train['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].fillna(train['ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'].mode()[0], inplace=True)
train['ë¶„ì•¼'].fillna(train['ë¶„ì•¼'].mode()[0], inplace=True)

#ë°ì´í„° ë¶„í• 
y = train['ì„±ê³µí™•ë¥ ']
X = train.drop(['ì„±ê³µí™•ë¥ ','ID'], axis=1)

#ì›í•« ì¸ì½”ë”©
X_dum = pd.get_dummies(X, columns = ['êµ­ê°€','ë¶„ì•¼','íˆ¬ìë‹¨ê³„','ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€','ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'], drop_first=True)

import numpy as np
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# ë°ì´í„° ì¤€ë¹„ (X_dumê³¼ yëŠ” ì´ë¯¸ ì¤€ë¹„ëœ ìƒíƒœë¡œ ê°€ì •)
# X_dum = pd.get_dummies(X, columns=['êµ­ê°€','ë¶„ì•¼','íˆ¬ìë‹¨ê³„','ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€','ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'], drop_first=True)

# 1. K-Fold Cross Validation ì„¤ì • (k=10)
kf = KFold(n_splits=10, shuffle=True, random_state=1)

# 2. Weighted MAE ê³„ì‚° í•¨ìˆ˜ ì •ì˜
def weighted_mae(y_true, y_pred, weights):
    abs_errors = np.abs(y_true - y_pred)
    return np.sum(abs_errors * weights) / np.sum(weights)

# 3. ëª¨ë¸ í›ˆë ¨ ë° êµì°¨ê²€ì¦ ìˆ˜í–‰
def cross_validate_model(X, y):
    fold_weights = []
    fold_errors = []

    # KFold êµì°¨ê²€ì¦ ìˆ˜í–‰
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # 4. ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
        rf = RandomForestRegressor(random_state=1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_val)

        # 5. ë¹ˆë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        y_val_rounded = y_val.round(2)  # ì†Œìˆ˜ì  ë‘ ìë¦¬ ë°˜ì˜¬ë¦¼
        value_counts = pd.Series(y_val_rounded).value_counts()
        freq_map = value_counts.to_dict()
        weights = np.array([1 / freq_map[val] for val in y_val_rounded])

        # 6. Weighted MAE ê³„ì‚°
        wmae = weighted_mae(y_val, y_pred, weights)
        fold_weights.append(weights)
        fold_errors.append(wmae)

    # 7. í‰ê·  ê°€ì¤‘ì¹˜ MAE ë°˜í™˜
    return np.mean(fold_errors)

# 8. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Bayesian Optimization)
search_space = {
    'n_estimators': Integer(100, 1000),  # íŠ¸ë¦¬ ê°œìˆ˜
    'max_depth': Integer(3, 20),          # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    'min_samples_split': Integer(2, 10),  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    'min_samples_leaf': Integer(1, 10),   # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
}

opt = BayesSearchCV(
    RandomForestRegressor(random_state=1),
    search_space,
    n_iter=50,  # íƒìƒ‰í•  íšŸìˆ˜
    cv=3,  # ë‚´ë¶€ êµì°¨ê²€ì¦ (3í´ë“œ)
    n_jobs=-1,  # ë³‘ë ¬ ì²˜ë¦¬
    scoring=None,  # scoringì€ Noneìœ¼ë¡œ ë‘ê³  ë‚´ë¶€ MAE ê³„ì‚°
    verbose=1,
)

# 9. ë² ì´ì§€ì•ˆ ìµœì í™” ìˆ˜í–‰ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •)
opt.fit(X_dum, y)

# 10. ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
print("Best Hyperparameters:", opt.best_params_)

# 11. ìµœì  ëª¨ë¸ë¡œ K-Fold êµì°¨ê²€ì¦ í›„ í‰ê°€
best_rf = opt.best_estimator_
final_wmae = cross_validate_model(X_dum, y)
print("Final Weighted MAE after Hyperparameter Optimization:", final_wmae)

# 12. ì¤‘ìš”ë„ 0.01 ì´ìƒì¸ í”¼ì²˜ë§Œ ì„ íƒ
importances = best_rf.feature_importances_
feature_names = X_dum.columns
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# 11. ì¤‘ìš”ë„ 0.01 ì´ìƒì¸ í”¼ì²˜ë§Œ ì„ íƒ
selected_features = importance_df[importance_df['Importance'] >= 0.01]['Feature'].tolist()
X_selected = X_dum[selected_features]

# 12. ì„ íƒëœ í”¼ì²˜ë¡œ ë‹¤ì‹œ êµì°¨ê²€ì¦ í‰ê°€
final_wmae_selected = cross_validate_model(X_selected, y)
print("Weighted MAE with Selected Features (Importance â‰¥ 0.01):", final_wmae_selected)

best_rf_selected = RandomForestRegressor(
    n_estimators=opt.best_params_['n_estimators'],
    max_depth=opt.best_params_['max_depth'],
    min_samples_split=opt.best_params_['min_samples_split'],
    min_samples_leaf=opt.best_params_['min_samples_leaf'],
    random_state=1
)
best_rf_selected.fit(X_selected, y)

# 13. test ë°ì´í„°ì— ëŒ€í•´ì„œë„ dummies ì²˜ë¦¬
test.drop('ID', axis=1, inplace=True)
X_test_dum = pd.get_dummies(test, columns=['êµ­ê°€','ë¶„ì•¼','íˆ¬ìë‹¨ê³„','ì¸ìˆ˜ì—¬ë¶€', 'ìƒì¥ì—¬ë¶€','ê¸°ì—…ê°€ì¹˜(ë°±ì–µì›)'], drop_first=True)

X_test_dum_1 = X_test_dum[selected_features]

# 14. test ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
y_test_pred = best_rf_selected.predict(X_test_dum_1)

# 15. sample_submission.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
sample_submission = pd.read_csv('C:\\Users\\USER\\Downloads\\ê¸°ì—…ì„±ê³µ\\sample_submission.csv')

# 16. ì˜ˆì¸¡ê°’ì„ 'ì„±ê³µí™•ë¥ ' ì»¬ëŸ¼ì— ì‚½ì…
sample_submission['ì„±ê³µí™•ë¥ '] = y_test_pred

# 17. ê²°ê³¼ ì €ì¥
sample_submission.to_csv('submissionì„±ê³µ12.csv', index=False)

print("ê²°ê³¼ê°€ 'submission.csv' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
